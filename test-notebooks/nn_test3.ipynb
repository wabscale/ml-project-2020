{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "df = pd.read_parquet('./frame.parquet.gzip')\n",
    "xdf = df[['title', 'tags', 'description', 'publish_hour', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'channel_title']]\n",
    "ydf = df[['category_id']]\n",
    "Xtr, Xts, ytr, yts = train_test_split(xdf, ydf, shuffle=True, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sequences(x):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    _sequences = tokenizer.texts_to_sequences(x)\n",
    "    maxlen = max(len(_seq) for _seq in _sequences)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    _xtr = tf.keras.preprocessing.sequence.pad_sequences(_sequences, maxlen, padding='post')\n",
    "    \n",
    "    return _xtr, maxlen, vocab_size\n",
    "\n",
    "def create_text_channel(maxlen, vocab_size):\n",
    "    input_ = tf.keras.layers.Input(shape=(maxlen,))\n",
    "    embedding = tf.keras.layers.Embedding(vocab_size, 1028)(input_)\n",
    "    conv = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='softmax')(embedding)\n",
    "    drop = tf.keras.layers.Dropout(0.05)(conv)\n",
    "    pool = tf.keras.layers.MaxPooling1D(pool_size=2)(drop)\n",
    "    flat = tf.keras.layers.Flatten()(pool)\n",
    "    \n",
    "    return input_, flat\n",
    "\n",
    "def create_numerical_channel(shape=(3,), activation='softmax'):\n",
    "    input_ = tf.keras.layers.Input(shape=shape)\n",
    "    dense = tf.keras.layers.Dense(8, activation=activation)(input_)\n",
    "    dense = tf.keras.layers.Dense(6, activation=activation)(input_)\n",
    "    #conv = tf.keras.layers.Conv1D(filters=10, kernel_size=4, activation='softmax')(dense)\n",
    "    drop = tf.keras.layers.Dropout(0.05)(dense)\n",
    "    flat = tf.keras.layers.Flatten()(drop)\n",
    "    \n",
    "    return input_, drop\n",
    "\n",
    "\n",
    "def create_model(maxlen_vocabsz, tchannels=3, nchannels=3, activation='softmax', loss='binary_crossentropy', optimizer='sgd', metric='categorical_accuracy'):\n",
    "    inputs = []\n",
    "    merge = []\n",
    "    \n",
    "    for maxlen, vocab_size in maxlen_vocabsz:\n",
    "        for _ in range(tchannels):\n",
    "            input_, flat = create_text_channel(maxlen, vocab_size)\n",
    "            inputs.append(input_)\n",
    "            merge.append(flat)\n",
    "            \n",
    "    for _ in range(nchannels):\n",
    "        input_, flat = create_numerical_channel(shape=(3,))\n",
    "        inputs.append(input_)\n",
    "        merge.append(flat)\n",
    "    \n",
    "    merged = tf.keras.layers.Concatenate()(merge)\n",
    "    dense = tf.keras.layers.Dense(10, activation=activation)(merged)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=activation)(dense)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_vocabsz = []\n",
    "Xtrain = []\n",
    "\n",
    "tchannels = 1\n",
    "for feat in ['title', 'tags', 'description', 'channel_title']:\n",
    "    _xtr, _maxlen, _vocab_size = convert_to_sequences(Xtr[feat].to_list())\n",
    "    maxlen_vocabsz.append((_maxlen, _vocab_size))\n",
    "    for _ in range(tchannels):\n",
    "        Xtrain.append(np.array(_xtr))\n",
    "\n",
    "nfeats = ['publish_hour', 'comments_disabled', 'ratings_disabled']\n",
    "nchannels = 3\n",
    "for _ in range(nchannels):\n",
    "    Xtrain.append(Xtr[nfeats].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(maxlen_vocabsz, tchannels=tchannels, nchannels=nchannels)\n",
    "model.fit(Xtrain, ytr.to_numpy(), epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 1590s 941ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 1598s 945ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 1596s 944ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 318s 188ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 318s 188ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 319s 188ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 315s 186ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 315s 186ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 315s 187ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 315s 186ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 316s 187ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 314s 186ms/step - loss: 26.4595 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 1679s 993ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 1680s 994ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 1678s 993ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 320s 189ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 320s 189ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 321s 190ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 314s 186ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 313s 185ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 313s 185ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 314s 185ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 311s 184ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 311s 184ms/step - loss: 26.4595 - accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 1688s 998ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 1693s 1s/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 1692s 1s/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 324s 191ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 325s 192ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 326s 193ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 315s 186ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 316s 187ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 315s 186ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 312s 184ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 312s 185ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 312s 185ms/step - loss: 26.4595 - binary_accuracy: 0.2440\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 1692s 1s/step - loss: -55.4853 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 1694s 1s/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 1694s 1s/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 318s 188ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 320s 189ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 321s 190ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 326s 193ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 326s 193ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 326s 193ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 321s 190ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 319s 189ms/step - loss: -55.4853 - categorical_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1691/1691 [==============================] - 320s 189ms/step - loss: -55.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 1/3\n",
      "1691/1691 [==============================] - 1719s 1s/step - loss: -55.4852 - accuracy: 0.2440\n",
      "Epoch 2/3\n",
      "1691/1691 [==============================] - 1724s 1s/step - loss: -55.4852 - accuracy: 0.2440\n",
      "Epoch 3/3\n",
      "1258/1691 [=====================>........] - ETA: 7:14 - loss: -55.4390 - accuracy: 0.2420"
     ]
    }
   ],
   "source": [
    "for loss in ['mse', 'binary_crossentropy', 'categorical_crossentropy', 'sparse_categorical_crossentropy']:\n",
    "    for metric in ['categorical_accuracy', 'accuracy', 'binary_accuracy']:\n",
    "        for optimizer in ['adam', 'sgd', 'adadelta', 'adagrad']:\n",
    "            m = create_model(maxlen_vocabsz, tchannels=tchannels, nchannels=nchannels, loss=loss, metric=metric, optimizer=optimizer)\n",
    "            m.fit(Xtrain, ytr.to_numpy(), epochs=3, batch_size=16)\n",
    "            m.save('model-{}-{}-{}.h5'.format(loss, metric, optimizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
